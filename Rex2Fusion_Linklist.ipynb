{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# FAST & ROBUST ECC TO EXCEL CONVERTER (FIXED REGEX)\n",
        "# ==============================================================\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(\"--- Step 1: Configuration ---\")\n",
        "\n",
        "# 1. Ask for Start ID\n",
        "default_id = 50\n",
        "start_id_input = input(f\"Enter Linklist Start Number (default {default_id}): \")\n",
        "try:\n",
        "    current_id = int(start_id_input)\n",
        "except ValueError:\n",
        "    current_id = default_id\n",
        "print(f\"-> Using Start ID: {current_id}\")\n",
        "\n",
        "# 2. Upload File\n",
        "print(\"\\n--- Step 2: Upload File ---\")\n",
        "print(\"Please upload your 'ExportECC' file (CSV or TXT)...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"X No file uploaded. Please run the cell again.\")\n",
        "else:\n",
        "    start_time = time.time()\n",
        "    filename = next(iter(uploaded))\n",
        "    print(f\"\\n-> Processing: {filename}...\")\n",
        "\n",
        "    # --- PART A: ROBUST FILE READING ---\n",
        "    # Read raw bytes to handle any encoding issues\n",
        "    raw_data = uploaded[filename]\n",
        "\n",
        "    # Try UTF-8 with BOM (common in Excel CSVs), then Latin-1\n",
        "    try:\n",
        "        content = raw_data.decode('utf-8-sig', errors='ignore')\n",
        "    except:\n",
        "        content = raw_data.decode('latin-1', errors='ignore')\n",
        "\n",
        "    # Clean hidden null bytes (common in generated files)\n",
        "    content = content.replace('\\x00', '')\n",
        "    lines = content.splitlines()\n",
        "\n",
        "    # --- PART B: FAST PARSING (REGEX) ---\n",
        "    collections = []\n",
        "    unit_map = {}\n",
        "\n",
        "    # Regex 1: Matches the Collection Name at the start of the line\n",
        "    # Looks for: Starts with #, captures everything until the first semicolon\n",
        "    name_pattern = re.compile(r\"^#([^;]+)\")\n",
        "\n",
        "    # Regex 2: Matches Codes and Quantities anywhere in the line\n",
        "    # Looks for: #, then (Code), then [, then (Qty), then ]\n",
        "    # [^\\[;#]+ means \"any character that is NOT [ or ; or #\"\n",
        "    code_pattern = re.compile(r\"#([^\\[;#]+)\\[([\\d\\.,]+)\\]\")\n",
        "\n",
        "    row_count = 0\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line: continue\n",
        "\n",
        "        # 1. Check if it is a Collection Line (Starts with #)\n",
        "        if line.startswith(\"#\"):\n",
        "            # A. Extract Name\n",
        "            name_match = name_pattern.search(line)\n",
        "            if name_match:\n",
        "                current_coll_name = name_match.group(1).strip()\n",
        "            else:\n",
        "                # Fallback if regex fails but line starts with #\n",
        "                parts = line.split(\";\")\n",
        "                current_coll_name = parts[0].replace(\"#\", \"\").strip()\n",
        "\n",
        "            # B. Extract All Codes at once (Faster)\n",
        "            found_codes = code_pattern.findall(line)\n",
        "\n",
        "            # If no codes found, we assume 0 units but still increment ID?\n",
        "            # (Adjust logic here if you want empty rows for empty collections)\n",
        "\n",
        "            for code, qty_str in found_codes:\n",
        "                # Clean Qty (replace comma with dot for decimals)\n",
        "                qty_str = qty_str.replace(\",\", \".\")\n",
        "                try:\n",
        "                    qty = float(qty_str)\n",
        "                    loop_count = int(qty) if qty.is_integer() else 1\n",
        "                except:\n",
        "                    loop_count = 1\n",
        "\n",
        "                # Add Rows\n",
        "                for _ in range(loop_count):\n",
        "                    collections.append({\n",
        "                        \"Linklist ID\": current_id,\n",
        "                        \"Linklist Name\": current_coll_name,\n",
        "                        \"Linklist Units\": code.strip(),\n",
        "                        \"Number of Units\": \"[1.000]\",\n",
        "                        \"Series ID\": 0\n",
        "                    })\n",
        "                    row_count += 1\n",
        "\n",
        "            # Increment ID for the next collection found\n",
        "            current_id += 1\n",
        "\n",
        "        # 2. Check if it is a Unit Line (Starts with ;#)\n",
        "        elif line.startswith(\";#\"):\n",
        "            # Clean: ;#UnitName; -> UnitName\n",
        "            unit_name = line.replace(\";\", \"\").replace(\"#\", \"\").strip()\n",
        "\n",
        "            if unit_name:\n",
        "                # Link to the PREVIOUS collection ID\n",
        "                active_id = current_id - 1\n",
        "\n",
        "                if unit_name not in unit_map:\n",
        "                    unit_map[unit_name] = []\n",
        "                unit_map[unit_name].append(active_id)\n",
        "\n",
        "    # --- PART C: BUILD EXCEL ---\n",
        "    print(f\"-> Parsing complete. Generating Excel with {row_count} collection rows...\")\n",
        "\n",
        "    df = pd.DataFrame(collections)\n",
        "\n",
        "    # Ensure Left Columns Exist (Fix for blank columns)\n",
        "    req_cols = [\"Linklist ID\", \"Linklist Name\", \"Linklist Units\", \"Number of Units\", \"Series ID\"]\n",
        "    for c in req_cols:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\" # Fill with empty string if no data found\n",
        "\n",
        "    # Prepare Right Columns (Units)\n",
        "    unit_data_list = []\n",
        "    for u_name, id_list in unit_map.items():\n",
        "        # Build Linked List String: <1~1~50|2~1~51|>\n",
        "        link_str = \"<\"\n",
        "        for idx, linked_id in enumerate(id_list):\n",
        "            link_str += f\"{idx+1}~1~{linked_id}|\"\n",
        "        link_str += \">\"\n",
        "\n",
        "        unit_data_list.append({\n",
        "            \"Unit Name\": u_name,\n",
        "            \"Linked Lists\": link_str\n",
        "        })\n",
        "\n",
        "    df_units = pd.DataFrame(unit_data_list)\n",
        "\n",
        "    # Merge Side-by-Side (Reset index to ensure they start at top)\n",
        "    final_df = pd.concat([df.reset_index(drop=True), df_units.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Final Column Order & Fill NaNs\n",
        "    final_cols = [\"Linklist ID\", \"Linklist Name\", \"Linklist Units\", \"Number of Units\", \"Series ID\", \"Unit Name\", \"Linked Lists\"]\n",
        "\n",
        "    # Add any missing columns to final DF\n",
        "    for c in final_cols:\n",
        "        if c not in final_df.columns:\n",
        "            final_df[c] = \"\"\n",
        "\n",
        "    final_df = final_df[final_cols]\n",
        "\n",
        "    # Fill any remaining NaNs (e.g., if one side is shorter than the other)\n",
        "    final_df = final_df.fillna(\"\")\n",
        "\n",
        "    # --- PART D: DOWNLOAD ---\n",
        "    output_filename = os.path.splitext(filename)[0] + \".xlsx\"\n",
        "    final_df.to_excel(output_filename, index=False)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\n[Success] File generated in {round(end_time - start_time, 2)} seconds.\")\n",
        "    print(f\"Downloading {output_filename}...\")\n",
        "    files.download(output_filename)"
      ],
      "metadata": {
        "id": "lwWqSfQHL379"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}